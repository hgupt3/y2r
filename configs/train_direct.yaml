# Training configuration for direct (non-diffusion) IntentTracker model
# This config references dataset_config.yaml for dataset and derived model parameters

# Dataset configuration
dataset_config: "dataset_config.yaml"  # Path to shared dataset config (relative to this file)

# Model configuration
model:
  model_type: 'direct'          # 'direct' for IntentTracker, 'diffusion' for DiffusionIntentTracker, 'autoreg' for AutoregressiveIntentTracker
  model_size: 's'               # 's' (small), 'b' (base), or 'l' (large) - sets hidden_size, num_heads, time_depth, vit_model
  track_type: '3d'              # '2d' or '3d' - must match dataset_config.yaml
  hand_mode: 'right'            # 'left', 'right', 'both', or null - must match dataset_config.yaml
  # Note: num_future_steps, frame_stack, text_mode are derived from dataset_config
  model_resolution: [256, 256]
  add_space_attn: true
  vit_frozen: false             # Fine-tune ViT (when unfreeze_after=0, this controls initial state)
  p_drop_attn: 0.1              # Attention dropout (0.0 = no dropout, 0.1-0.3 typical)
  
  # Diffusion-specific parameters (only used when model_type='diffusion')
  num_diffusion_steps: 100      # Number of diffusion training timesteps
  num_inference_steps: 10       # Number of inference steps (DDIM sampling)
  beta_schedule: 'squaredcos_cap_v2'  # Noise schedule: 'linear', 'squaredcos_cap_v2', etc.

# Training configuration
training:
  # Device configuration
  device: 'cuda:0'                # Device to use: 'cuda', 'cuda:0', 'cuda:1', 'cpu', etc.
  
  # Training loop
  num_epochs: 150
  batch_size: 64                # Batch size (adjust based on GPU memory)
  num_workers: 12                # DataLoader workers (adjust based on CPU cores)
  
  # Optimizer
  lr: 1.0e-4
  weight_decay: 1.0e-5
  grad_clip: 1.0
  
  # Modern optimizations
  use_amp: true                 # Mixed precision training
  use_compile: true             # torch.compile for speed
  ema_decay: 0.999              # Model EMA
  
  # Encoder unfreezing schedule (curriculum learning for ViT + SigLIP)
  unfreeze_after: 0.3           # Unfreeze encoders at 30% of training (0.0 = always unfrozen)
  
  # Learning rate schedule (cosine with warmup)
  lr_schedule: "cosine"
  warmup_epochs: 5              # 5% of total epochs
  min_lr: 1.0e-6                # Minimum LR for cosine decay
  
  # Validation
  val_every_n_epochs: 5
  val_vis_samples: 64            # Number of samples to visualize
  
  # Checkpointing
  checkpoint_dir: "./checkpoints"  # Base directory (timestamped subdirs will be created)
  
  # Logging
  wandb_project: "intent-tracker"
  wandb_entity: null            # Your W&B username
  log_every_n_steps: 10
  log_gradients: true           # Log gradient norms
  log_params: true              # Log parameter histograms (expensive, maybe every 100 steps)
